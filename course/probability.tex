% \documentclass{book}

\documentclass[12pt]{article}
\usepackage[pdfborder={0 0 0.5 [3 2]}]{hyperref}%
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}%
\usepackage[shortalphabetic]{amsrefs}%
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{amssymb}                
\usepackage{amsmath}                
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage[table,xcdraw]{xcolor}
\usepackage{tikz}
\usepackage{float}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{mathtools}
\usepackage{cool}
\usepackage{url}
\usepackage{graphicx,epsfig}
\usepackage{framed}
\usepackage{hyperref}  

\usetikzlibrary{automata,arrows,positioning,calc}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{exercise}{Exercise}%
\newtheorem{problem}[exercise]{Problem}%
\newtheorem*{example}{Example}

\theoremstyle{remark}
\newtheorem*{question}{Question}
\newtheorem*{observation}{Observation}
\newtheorem*{remark}{Remark}

\graphicspath{ {images/} }

\setlength{\parindent}{0cm}
\renewcommand{\vec}[1]{\ensuremath{\mathbf{#1}}}

\def\noi{\noindent}
\def\T{{\mathbb T}}
\def\R{{\mathbb R}}
\def\N{{\mathbb N}}
\def\C{{\mathbb C}}
\def\Z{{\mathbb Z}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\Q{\mathbb{Q}}
\def\ind{{\mathbb I}}

\def\cale{{\mathcal E}}
\def\cals{{\mathcal S}}
\def\calc{{\mathcal C}}
\def\caln{{\mathcal N}}
\def\calb{{\mathcal B}}
\def\calg{{\cal G}}

\def\ds{\displaystyle}
\def\ra{\rightarrow}
\newcommand{\conv}{\mbox{\rm conv}}
\newcommand{\spaan}{\mbox{\rm span}}
\newcommand{\deet}{\mbox{\rm det}}
\newcommand{\aff}{\mbox{\rm aff}}
\newcommand{\cl}{\mbox{\rm cl}}
\newcommand{\dimm}{\mbox{\rm dim}}
\newcommand{\sm}{\setminus}
\def\ci{\perp\!\!\!\perp}


\newcommand{\ink}{\rule{.5\baselineskip}{.55\baselineskip}}

\begin{document}

\section{Probability Essentials}
In general parlance, the term \emph{probability} is used as a measure of a person's belief in the occurrence of an event. This is the \emph{subjective} notion of probability. Take a sentence such as, ``There is a 60\% probability of rain tomorrow''. Where did this come from? A professional meteorologist has used a sophisticated mathematical model to distill large amounts of atmospheric data down to a simple number indicating her belief that it is more likely to rain than to not rain tomorrow. We can intepret this number however we wish. We should be careful, however, to examine both our biases and those of the meterologist. Who does this meterologist work for? How reliable have their predictions been in the past? (We should be careful here since we might be more likely to recall rain when none was predicted than the other way around.) Are meterologists more likely to err on the side of predicting rain, since that way fewer people will be upset if they are wrong? We can use this number to make important life decisions such as whether or not to carry an umbrella. The downside of this is that it is in essence a subjective opinion, even if that opinion comes from an expert. In addition, for subjective probabilites to make sense, they have to be internally consistent. In this example, if we believe there is a 60\% chance of rain tomorrow, we must also believe that there is a 40\% chance of it not raining tomorrow.
\\

Let's look at a different example. What is the probability of rolling a 1 on a standard six-sided die? We can argue based on symmetry that since dice are cubical, there should be no reason for one face to be preferred over another. Thus this probability is 1/6. This is the \emph{classical} notion of probability. In this interpretation, we use symmetry arguments to divide our experiment (a single die roll, in this case) into elementary events which are equally probable (the six distint die rolls). We can compute the proability of any event using these elementary, equiprobably events. The limitation of this approach is that is requires a symmetry argument to be effective; thus it works for coin flips and die rolls, but not for more complicated scenarios.\\

We can look at the die roll experiment in another way. Imaging rolling a standard six-sided die repeatedly. The empirical probability of rolling a 1 is the ratio of the number of times a 1 is rolled to the total number of rolls. In general, we have:
\[
\text{empirical probability of a certain event} = \frac{ \text{number of times the event occurs }}{\text{total number of trials}}
\]
Intuitively, as we perform more and more dice rolls, the empirical probability of rolling a 1 should approach some mythical quantity which we call the \emph{true probability} of rolling a 1. This approach is the \emph{empirical}, or \emph{frequentist}, approach to probability. For a standard, six-sided die, it stands to reason that the empirical probabilty should approach the classical probabiliy of 1/6 as the number of rolls approaches infinity. This result is called the \emph{law of large numbers} and will be discussed later in the course. The empirical approach has a critical advantage over the classical approach in that we do not require symmetry to compute our probabilities. As an example of this, think of how you would determine the probability of rolling a 1 if someone handed you a loaded die. The empirical approach does, however, have its limits. Returing to the weather example, there is no way to think of the chance of rain tomorrow as the limit of a sequence of independent experiements. Unless we are in the movie Groundhog Day, tomorrow can only happen once!\\

For our purposes, we require a more rigorous, mathematical construction of probability. This is known as \emph{axiomatic probability} and will unify some of the aspects of the other approaches to probability. For this, we turn to the language of \emph{set theory}.

\subsection{Sample Spaces}
A \emph{set} is a collection of distinct objects. A \emph{sample space}, denoted $\cals$ is the set of all outcomes of a particular experiment. Here are some examples of sample spaces:
\begin{enumerate}
\item Single coin flip: $\cals = \{H, T\}$
\item Roll of one standard, six-sided die: $\cals = \{1, 2, 3, 4, 5, 6\}$
\item Roll of two standard, six-sided dice: Here we represent the sample spaces as ordered pairs.

\begin{figure}[H]
\centering
% \caption{My caption}
\label{twodice}
\begin{tabular}{llllllll}
           &                        &        &        & \multicolumn{2}{l}{second roll} &        &                             \\
           &                        & 1      & 2      & 3              & 4              & 5      & 6                           \\ \cline{3-8} 
           & \multicolumn{1}{l|}{1} & (1, 1) & (1, 2) & (1, 3)         & (1, 4)         & (1, 5) & \multicolumn{1}{l|}{(1, 6)} \\
           & \multicolumn{1}{l|}{2} & (2, 1) & (2, 2) & (2, 3)         & (2, 4)         & (2, 5) & \multicolumn{1}{l|}{(2, 6)} \\
first roll & \multicolumn{1}{l|}{3} & (3, 1) & (3, 2) & (3, 3)         & (3, 4)         & (3, 5) & \multicolumn{1}{l|}{(3, 6)} \\
           & \multicolumn{1}{l|}{4} & (4, 1) & (4, 2) & (4, 3)         & (4, 4)         & (4, 5) & \multicolumn{1}{l|}{(4, 6)} \\
           & \multicolumn{1}{l|}{5} & (5, 1) & (5, 2) & (5, 3)         & (5, 4)         & (5, 5) & \multicolumn{1}{l|}{(5, 6)} \\
           & \multicolumn{1}{l|}{6} & (6, 1) & (6, 2) & (6, 3)         & (6, 4)         & (6, 5) & \multicolumn{1}{l|}{(6, 6)} \\ \cline{3-8} 
\end{tabular}
\end{figure}

\item Number of free throw attempts it takes for me to make a single basket: $\cals = \{1, 2, 3, ...\}$. This set is often denoted $\N$ for the natural numbers (positive integers)
\item Number of minutes late my RIPTA bus arrives: $\cals = [0, \infty)$. 
\end{enumerate}
Note that the first three sample spaces contain only a finite number of elements (2, 6, and 36 elements, respectively). These are called \emph{finite sample spaces}. The fourth and fifth sample spaces both contain an infinite number of elements, but there is a fundamental difference between the two. The set $\N$ can be written out in its entirety in an infinitely long list; another way to think about this is that we can start at 1 and count up to any number in the set (as long as we have enough time!). A set with this property is called \emph{countable}. For the set $[0, \infty)$, it makes intuitive sense that we cannot do this, i.e. we cannot list all the elements and, say, ``count up to $\pi$''. A proof of this fact is left for another course. Such an infinite set is called \emph{uncountable}\footnote{An uncountable set is a ``larger infinity'' than a countable set, which leads to the concept of ``sizes of infinity''. John Green alludes to this in his novel \emph{The Fault in Our Stars}, but unforunately gets the math wrong. If you find this interesting, I recommend the Vi Hart video \url{https://www.youtube.com/watch?v=23I5GS4JiDg}}. A sample space which is either finite or countable is called \emph{discrete}.\\

\subsection{Events and Subsets}
An \emph{event} is a subset of a sample space. Events are usually designated by capital letters, and we write the relationship ``$A$ is a subset of $\cals$'' by $A \subset \cals$. For two events $A$ and $B$, $A \subset B$ if every element in $A$ is also contained in $B$. The \emph{empty set}, denoted $\emptyset$, is the set containing no elements, and it is a subset of every set.
\\

Let us consider the sample space $\cals = \{1, 2, 3, 4, 5, 6\}$, representing the roll of a single die. The following are examples of events:
\begin{enumerate}
\item $A = \{2, 4, 6\}$, the event that an even number is rolled
\item $B = \{1, 2, 3\}$, the event that the roll is less than or equal to 3
\item $C = \{1\}$, the event that a 1 is rolled
\end{enumerate}
The event $C$ consists of a single element in the sample space. Such an event is called a \emph{simple event} and cannot be decomposed. The events $A$ and $B$ are each composed of three simple events.\\

Next, consider the sample space repesenting rolls of two dice. Let $E$ be the event that the sum of the two dice is 7. We can represent this event graphically; in the figure below, the event $E$ consists of the squares which are highlighted in yellow.

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{figure}[H]
\centering
% \caption{My caption}
\label{two-dice-sum-seven}
\begin{tabular}{llllllll}
           &                        &                                                     &                                                     & \multicolumn{2}{l}{second roll}                                                                           &                                                     &                                                     \\
           &                        & 1                                                   & 2                                                   & 3                                                   & 4                                                   & 5                                                   & 6                                                   \\ \cline{3-8} 
           & \multicolumn{1}{l|}{1} & (1, 1)                                              & (1, 2)                                              & (1, 3)                                              & (1, 4)                                              & \multicolumn{1}{l|}{(1, 5)}                         & \multicolumn{1}{l|}{\cellcolor[HTML]{F8FF00}(1, 6)} \\ \cline{7-8} 
           & \multicolumn{1}{l|}{2} & (2, 1)                                              & (2, 2)                                              & (2, 3)                                              & \multicolumn{1}{l|}{(2, 4)}                         & \multicolumn{1}{l|}{\cellcolor[HTML]{F8FF00}(2, 5)} & \multicolumn{1}{l|}{(2, 6)}                         \\ \cline{6-7}
first roll & \multicolumn{1}{l|}{3} & (3, 1)                                              & (3, 2)                                              & \multicolumn{1}{l|}{(3, 3)}                         & \multicolumn{1}{l|}{\cellcolor[HTML]{F8FF00}(3, 4)} & (3, 5)                                              & \multicolumn{1}{l|}{(3, 6)}                         \\ \cline{5-6}
           & \multicolumn{1}{l|}{4} & (4, 1)                                              & \multicolumn{1}{l|}{(4, 2)}                         & \multicolumn{1}{l|}{\cellcolor[HTML]{F8FF00}(4, 3)} & (4, 4)                                              & (4, 5)                                              & \multicolumn{1}{l|}{(4, 6)}                         \\ \cline{4-5}
           & \multicolumn{1}{l|}{5} & \multicolumn{1}{l|}{(5, 1)}                         & \multicolumn{1}{l|}{\cellcolor[HTML]{F8FF00}(5, 2)} & (5, 3)                                              & (5, 4)                                              & (5, 5)                                              & \multicolumn{1}{l|}{(5, 6)}                         \\ \cline{3-4}
           & \multicolumn{1}{l|}{6} & \multicolumn{1}{l|}{\cellcolor[HTML]{F8FF00}(6, 1)} & (6, 2)                                              & (6, 3)                                              & (6, 4)                                              & (6, 5)                                              & \multicolumn{1}{l|}{(6, 6)}                         \\ \cline{3-8} 
\end{tabular}
\end{figure}

\subsection{Basic Set Operations}

\begin{quotation}
``You have multiple core competencies with surprisingly minimal Venn. You can pivot from working on astrophysics problems, to teaching the young Arkers, to podcasting to folks on the ground, without skipping a beat!'' - Neal Stephenson, \emph{Seveneves}
\end{quotation}

Let $\cals$ be our sample space, the set of all elements under consideration. Consider two events $A$ and $B$ which are subsets of $\cals$. We have the following three basic set operations, which are handily illustrated using Venn diagrams.
\begin{enumerate}
\item The \emph{union} of $A$ and $B$, denoted $A \cup B$, is the set of all elements which are in $A$ or $B$ (or both). That is, the union is all elements that are in at least one of the two sets.
\begin{figure}[H]
\centering
\includegraphics[width=8cm]{AunionB}
\end{figure}
\item The \emph{intersection} of $A$ and $B$, denoted $A \cap B$, is the set of all elements which are in both $A$ and $B$.
\begin{figure}[H]
\centering
\includegraphics[width=8cm]{AintersectB}
\end{figure}
Two sets $A$ and $B$ are \emph{disjoint} or \emph{mutually exclusive} if they have no elements in common, i.e. if $A \cap B = \emptyset$.
\begin{figure}[H]
\centering
\includegraphics[width=8cm]{exclusive}
\end{figure}
\item The \emph{complement} of $A$, denoted $A^c$, is the set of all points in $\cals$ which are not in $A$. Note that $A$ and $A^c$ are disjoint, and $A \cup A^c = \cals$.
\begin{figure}[H]
\centering
\includegraphics[width=8cm]{Acomplement}
\end{figure}
\end{enumerate}
There are many relationships between these operations which fall under the rubric of set algebra. Most of them we will not need, but we mention a few useful ones here:
\begin{enumerate}
\item Distributive laws
\begin{align*}
A \cap (B \cup C) &= (A \cap B) \cup (A \cap C) \\
A \cup (B \cap C) &= (A \cup B) \cap (A \cup C)
\end{align*}
\item DeMorgan's laws
\begin{align*}
(A \cap B)^c &= A^c \cup B^c &\text{(the complement of an intersection is the union of the complements)}\\
(A \cup B)^c &= A^c \cap B^c &\text{(the complement of a union is the intersection of the complements)}
\end{align*}
\end{enumerate}

\subsection{Axiomatic Definition of Probability}
Equipped with our our knowlege of set theory, we can define probability axiomatically as follows. Given any event $A$ in our sample space $\cals$, we assign a probability $\P(A)$ to that event such that the following rules hold\footnote{You can construct a coherent notion of probabiltiy with fewer axioms and derive the remaining rules from these; I like this version of probability rules, since it codifies what we want to be true given our intuitive notion of probability.}:
\begin{enumerate}
\item $0 \leq \P(A) \leq 1$ \\

The probability of an event is a real number between 0 and 1, where a probability of 0 means that the event will never occur, and a probability of 1 means that the event will always occur.
\item $\P(\emptyset) = 0$ \\

The probability that nothing happens is 0, i.e. something must happen.
\item $\P(\cals) = 1$ \\

The probability of the whole sample space is 1, which is another way of saying that something must happen.
\item If $A \subset B$, then $\P(A) \leq \P(B)$ \\

If we make a set bigger, its probability can only increase (or stay the same); it cannot decrease.

\item If $A_1, A_2, \dots, A_n$ are pairwise disjoint events, i.e. $A_i \cap A_j = \emptyset$ if $i \neq j$, then
\[
\P(A_1 \cup A_2 \cup \cdots \cup A_n) = \sum_{i=1}^n \P(A_i)
\]
This holds for an infinite sequence as well, i.e. if $A_1, A_2, A_3, \dots$ are a sequence of pairwise disjoint events, then 
\[
\P(A_1 \cup A_2 \cup A_3 \cup \cdots) = \sum_{i=1}^\infty \P(A_i)
\]
\end{enumerate}

From these we can derive a very important rule:
\[
\P(A) + \P(A^c) = 1, \:\:\text{ i.e. }\P(A) = 1 - \P(A^c)
\]
Sometimes it is easier to calculate the probability of an event \emph{not} happening than the probability of the event itself!\\

These rules tell us the properties that we want probability to have. However, given a sample space, they do not actually tell us how to assign probabilities to each event in the sample space. Doing that in a way that is consistent with the above rules can be a bit tricky \footnote{In fact, for an uncountable sample space such as $\cals = [0, 1]$, you can show that you cannot construtct a notion of probability which is consistent with all the rules; this is the starting point for the development of measure theory, and is beyond the scope of this course.}, but luckily for a discrete sample space we can do this with no problem. Since a discrete sample space is composed of a finite (or countable) number of simple events, all we have to do is assign probabilities to each simple event in such a way that they all add up to 1.

\begin{example}Consider once again tossing a single die. The sample space for this is $\cals = \{1, 2, 3, 4, 5, 6\}$. This sample space contains 6 simple events: $\{1\}, \{2\}, \{3\},\{4\}, \{5\}$, and $\{6\}$. We can assign any probabilties we want to these simple events, as long as they add up to 1. For example, assuming we have a fair die, we can let $\P(\{i\}) = 1/6$ for $i = 1, \dots, 6$. If we like, we can check that all the above rules hold. If we have a loaded die, which rolls a 6, say, half the time, we could assign probabilites: $\P(\{6\}) = 1/2, \P(\{i\}) = 1/10$ for $i = 1, \dots, 5$.
\end{example}

\begin{example}Consider this time a countable sample space $\cals = \N = \{1, 2, 3, \dots\}$. One possibility is to assign probabilities $\P(\{i\}) = 1/2^i$ for $i = 1, 2, 3, \dots$, i.e. $\P(\{1\}) = 1/2$, $\P(\{2\}) = 1/4$, $\P(\{3\}) = 1/8$, etc. Perhaps you recall from calculus that this is a geometric series with first element $1/2$ and common ratio $1/2$, and so we know its sum is:
\[
\sum_{i = 1}^{\infty} \P(\{i\}) = \sum_{i = 1}^{\infty} \frac{1}{2^i} = \frac{1}{2}\frac{1}{1 - 1/2} = 1
\]
Since the sum of the probabilities of all the simple events is 1, we are all set! If you have not seen this before, we will cover this in more detail when we discuss the geometric distribution. In the meantime, here is a nice picture to convince you that the sum is indeeed 1.
\begin{figure}[H]
\centering
\includegraphics[width=5cm]{Geometric_series}
\end{figure}
\end{example}

% end of class 1
% \end{document}

\subsection{Discrete Uniform Distribution}
The first probability distribution we will consider is the uniform distribution on a \emph{finite} sample space. In the discrete uniform distribution, every simple event is equally likely to occur. If we have a finite sample space with $n$ simple events, then the discrete uniform distribution assigns each simple event a probability of $1/n$. Why is this the case? If each simple event has probability $p$ and there are $n$ simple events $A_1, \dots A_n$ in our sample space, then using the fact that the sample space $S$ must have probability 1 and that the simple events are pairwise disjoint:
\begin{align*}
1 &= \P(S) \\
&= \P(A_1 \cup A_2 \cup \cdots \cup A_n) \\
&= \P(A_1) + \P(A_2) + \dots + \P(A_n) \\
&= np
\end{align*}
where in the last line we use the fact that the simple events each have probability $p$, and there are $n$ of them. Solving for $p$, we get $p = 1/n$.

\begin{example}Consider the finite sample space $\cals$ representing the roll of two standard dice. This sample space has 36 simple events, so each simple event has a probability of 1/36. Note that for the simple events in this sample space, the order of the die rolls matters. $(1, 6)$ and $(6, 1)$ are two different events; even though both events contain the same die rolls, for the former, the 1 is rolled first, while for the latter, the 6 is rolled first.
\begin{enumerate}
\item What is the probability that the sum of the two dice is 7? Looking at the graphical depiction of this event above, we see that there are 6 simple events that give us a sum of 7. Thus the probability of a sum of 7 is 6/36 = 1/6.
\item What is the probability that the sum of the two dice is less than 11. In this case, it is easier to compute the probability that the sum is 11 or greater and then subtract that probability from 1. (Why can we do this?) Let $A$ be the event that the sum is 11 or greater. $A$ is composed of 3 simple events: $(6, 6), (6, 5)$, and $(5, 6)$. Thus we have $\P(A) = 3/36$. The probability we want is:
\[
\P(A^c) = 1 - \P(A) = 1 - 3/36 = 33/36
\]
\end{enumerate}
\end{example}
In the previous example, it is relatively straightforward to draw the sample space, so we can essentially compute any probabily we want simply by looking at the picture and counting which boxes comprise our event of interest. For more complicated problems, this is not as easy. Consider the following example:

\begin{example}A communication system consists of $n$ antennas arranged in a line. Exactly $m$ out of the $n$ antennas are defective. The system is functional if no two consecutive antennas are defective. Assuming that each linear arrangement of the antennas is equally likely, what is the probability that the system will be functional?\\

For small values of $n$ and $m$, say $n = 4$ and $m = 2$, we can write out all of the possible configurations. Representing a functional antenna by \texttt{1} and a defective antenna by \texttt{0}, there are exactly six linear arrangements:
\begin{enumerate}[noitemsep]
\item 0 1 1 0
\item 0 1 0 1
\item 0 0 1 1
\item 1 0 0 1
\item 1 0 1 0
\item 1 1 0 0
\end{enumerate}
Take a moment to convince yourself that these are the only possible configurations. Configurations 1, 2, and 5 are functional, so there are 3 functional configurations out of 6 total configurations. So in this case, the probability that the system is functional is 3/6. \\

For general $n$ and $m$, it is not immediately obvious how to perform the requisite counting of configurations. Taking a cue from the Count on Sesame Street, we need to learn more about counting. The mathematical theory of counting is known as \emph{combinatorics}.
\end{example}

\subsubsection{The Basic Principle of Counting}

\begin{framed}
  \emph{The basic principle of counting ($mn$-rule) }\\
  \rule{\dimexpr\linewidth-2\fboxsep-2\fboxrule}{.1pt} \\
  Suppose we are performing two experiments. If the first experiment has $m$ possible outcomes and the second experiment has $n$ possible outcomes, there are $mn$ possible outcomes for the two experiments. We can generalize this to more than two experiments; in that case, we take the product of the number of outcomes from each experiment.
\end{framed}

To see this, draw a grid of boxes with the $m$ outcomes from the first experiment on the left and the $n$ outcomes of the second experiment across the top. There are $mn$ total boxes, which are all the possible outcomes of both experiments.

\begin{figure}[H]
\centering
\includegraphics[width=6cm]{mnrule.eps}
\end{figure}

We have already seen the basic principle of counting in play when we looked at the sample space for two dice. Each of the two die rolls is a separate experiement. Since there are 6 outcomes for each die roll, the total number of outcomes is $6 \cdot 6 = 36$. We can extend this to three or more dice. For three dice, for example, the total number of oucomes is $6 \cdot 6 \cdot 6 = 6^3 = 216$. If you like, you can visualize this as a cube. For $n$ dice, there are $6^n$ possible outcomes. I have trouble visualizing this for $n > 3$, but perhaps you can.\\

We can also think of the basic principle of counting in terms of choosing items from groups. If there are $m$ items in Group 1 and $n$ items in Group 2, there are $mn$ pairs of items consisting of one item from each group. Again, this can be extended to any number of groups.\\

\begin{example}When I was growing up in Virginia, standard (non-vanity) license plates were composed of three letters (A-Z) followed by three digits (0-9). Assuming that all such possibilities can exist:
\begin{enumerate}
\item How many possible license plates are there? \\

We are choosing items from 6 groups. The first three groups contain 26 items, and the last three groups contain 10. Thus the number of possibilities is: $26 \cdot 26 \cdot 26 \cdot 10 \cdot 10 \cdot 10 = 17,576,000$. (That works out to a little more than two cars per person.)
\item If the Virginia DMV decided that letters and numbers could not be repeated, how many possible license plates would there be?\\

The difference here is that the group sizes shrink as we choose items. For the letters, the first group has 26 items. The second group only has 25 items, since we cannot choose the letter we chose from the first group. The third group has 24 items, since we cannot choose the letter we chose from the first two groups. The digits are similar. Thus the number of possibilities is: $26 \cdot 25 \cdot 24 \cdot 10 \cdot 9 \cdot 8 = 11,232,000$
\end{enumerate}
\end{example}

\subsubsection{Permutations}

\begin{framed}
An ordered arrangement of $r$ distinct items is called a \emph{permutation}. The number of ways of ordering $r$ items drawn from a group of $n$ distinct items is designated $nPr$.
\end{framed}

Before we give the formula for the number of possible permutations, let's look at some examples so we can get an intuitive understanding of what is going on.

\begin{example}How many different ordered arrangements are there of the letters \emph{a}, \emph{b}, and \emph{c}? \\

In this case, we can actually write them all out: \emph{abc}, \emph{acb}, \emph{bac}, \emph{bca}, \emph{cab}, \emph{cba}.\\
From this we see that there are 6 possible ordered arrangements. We can also do this using the ``choosing-from-groups'' approach. For the first letter, we have 3 to choose from; for the second, we have only 2; and for the third, there is only one remaining letter. Thus the number of permutations is:
\[
3 \cdot 2 \cdot 2 = 3! = 6
\]
Similarly, the number of ordered arrangements of $n$ distinct symbols is:
\[
n \cdot (n-1) \cdot (n-2) \cdots 2 \cdot 1 = n!
\]
The symbol $n!$ is the \emph{factorial} operation, and it is defined exactly as written above.
\end{example}

\begin{example}Every December (since 1996), the city of Ithaca, NY hosts the International Rutabega Curl\footnote{\url{http://www.rutabagacurl.com}} (contestants must supply their own rutabega). Gold, silver, and bronze medals are given to the top three finishers. If 100 contentants enter the rutabega curl, how many possibilities are there for the winners?\\

For the gold medalist, we have 100 contestants to choose from. Since you cannot win more than one medal, we choose from 99 contentants for the silver medal and 98 contestants for the bronze medal. The number of medal possibilities is:
\[
100 \cdot 99 \cdot 98 = 970,200
\]
\end{example}

We can generalize this in the following formula:

\begin{framed}
The number of permutations (ordered arrangements) of $r$ items drawn from a group of $n$ distinct items is:
\[
nPr = n (n-1)(n-2)\cdots(n - r + 1) = \frac{n!}{(n-r)!}
\]
\end{framed}
To see this, there are:
\begin{itemize}
\item $n$ ways to choose the first item
\item $n-1$ ways to choose the second item\\ \\$\cdots$
\item $n - r + 1$ ways to choose the $r$th item
\end{itemize}
Multiply these together to get the permutation formula. To get the ``factorial form'' of the permutation formula, we have:
\[
nPr = n (n-1)(n-2)\cdots(n - r + 1)\frac{(n-r)!}{(n-r)!} = \frac{n!}{(n-r)!}
\]
where $n! = n(n-1)\cdots2\cdot1$, and we define $0! = 1.$\footnote{This may seem a little arbitrary, but it is convenient. It also makes some sense that there is exactly one way to arrange no items.}

\subsubsection{Combinations}
Consider the following example, which is modification of the Rutabega Curl example above.

\begin{example}100 students buy raffle tickets. Three names are chosen from a hat uniformly at random to win a free sandwich at Eastside Pockets\footnote{A popular eatery on Thayer St. near Brown University}. How many possibilities are there for the winners?\\

How is this problem fundamentally different from the Rutabega Curl? Here, the three prizes are identical, as opposed to the three distinct medals in the Rutabega Curl. We can think of this problem as selecting 3 items from a group of 100, but \emph{the order in which we select them does not matter}. Let's look at this in a few stages:
\begin{enumerate}
\item Let's pretend for a moment that the order of selection matters. Then, as in the Rutabega Curl, there are $100 \cdot 99 \cdot 98$ possibilities.
\item Compared to the the case where order matters, is the number of possibilities greater, fewer, or the same? There must be fewer possibilities since, for example, choosing the numbers $(1, 2, 3)$ from the hat in that order is the same as choosing them in the order $(3, 2, 1)$.
\item How many permutations correspond to choosing the numbers 1, 2, and 3 from the hat? We can write out all the permutations:
\[
(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), \text{ and } (3, 2, 1)
\]
This gives us a total of 6 permutations. But this number is $3!$, the number of ordered arrangements of 3 objects. Does this make sense that this should be the case.
\item Since we have shown that 6 permutations correspond to one possibility of winners, all we need to do is divide the number of permutations by 6. Thus the total number of possibilities for the winners is:
\[
\frac{100 \cdot 99 \cdot 98}{6}
\]
\end{enumerate}
\end{example}

\begin{framed}
An unordered arrangement of $r$ distinct items is called a \emph{combination}. The number of combinations of $r$ items drawn from a group of $n$ distinct items is denoted $\binom{n}{r}$ (or sometimes $nCr$). We can think of this as:
\begin{enumerate}
\item The number of subsets of size $r$ which can be formed from a group of $n$ distinct objects
\item The number of ways to select $r$ items from a group of $n$ distinct items, where order does not matter
\end{enumerate}
\end{framed}

Using the logic from the previous example, we can deduce the formula for $\binom{n}{r}$. All we have to do is take the number of permutations and divide by $r!$, which is the number of ordered arrangements (permutations) of $r$ items.

\begin{framed}
The number of combinations (unordered arrangements) of $r$ items drawn from a group of $n$ distinct items is:
\[
\binom{n}{r} = \frac{nPr}{r!} = \frac{n!}{r!(n-r)!}
\]
\end{framed}

\begin{example}The Brown crossword puzzle club consists of 5 undergraduate and 7 graduate students.
\item How many different committees of 2 undergrads and 3 grad students can be formed?

There are $\binom{5}{2}$ possible groups of 2 undergrads, and $\binom{7}{3}$ possible groups of 3 grad students. We multiply these together (basic principle of counting) to get:
\[
\binom{5}{2} \binom{7}{3}  = \frac{5!}{2!3!} \frac{7!}{3!4!} = \frac{5\cdot4}{2\cdot1} \frac{7\cdot6\cdot5}{3\cdot2\cdot1} = 10 \cdot 35 = 350
\]

\item What if two of the grads students refuse to be on the same committee?

There are $\binom{7}{3} = 35$ possible groups of 3 graduate students. How many contain both of the two students who refuse to serve together? To make a three-person committee which includes these two, you need to include the two rival students plus one other student selected from the five remaining graduate students. Thus there are $\binom{2}{2}\binom{5}{1} = 5$ committees which include the two rival students. Subtracting from 35, there are 30 committees which don't include both rival students. As above, we multiply to get $10 \cdot 30 = 300$ possible committees.
\end{example}

Computing probabilities of poker hands are classic problems in probability. Since the order of the cards in a poker hand does not matter, these problems involve combinations.

\begin{example}
You are dealt a five-card poker hand. What is the probability of getting the following hands:
\begin{enumerate}
\item A full house (three of cards of one number plus two cards of another number)\\

There are 52 cards in a poker deck, and a poker hand is 5 cards. Since the order of the cards dealt does not matter, there are $\binom{52}{5}$ possible five-card poker hands. To count the number of hands which give us a full house, we use the following procedure:
\begin{enumerate}
\item Choose the number for the three-of-a-kind. There are $\binom{13}{1}$ ways to do that. Now select three out of the four cards of this number. There are $\binom{4}{3}$ ways to do that. Thus there are $\binom{13}{1}\binom{4}{3}$ ways to choose the three-of-a-kind.
\item Choose the number for the pair. There are $\binom{12}{1}$ ways to do that, since we have already chosen one of the numbers for the three-of-a-kind. Now select two out of the four cards of this number. There are $\binom{4}{2}$ ways to do that. Thus there are $\binom{12}{1}\binom{4}{2}$ ways to choose the pair once the three-of-a-kind has been chosen.
\item Multiply all of these together to get $\binom{13}{1}\binom{4}{3}\binom{12}{1}\binom{4}{2}$ poker hands which are full houses.
\end{enumerate}
To get the probability of a full house, we divide by the number of poker hands (the size of the sample space) to get:
\[
\frac{\binom{13}{1}\binom{4}{3}\binom{12}{1}\binom{4}{2}}{\binom{52}{2}} = 0.00144
\]
\end{enumerate}
It is worth noting that the number of full houses is not $\binom{13}{2}\binom{4}{3}\binom{4}{2}$, i.e. choose two numbers, then choose a three-of-a-kind, then choose a pair. Why does this not work? This method does not distinguish betwen, say, \texttt{33366} and \texttt{33666} (i.e. it treats these as the same), and these are two distinct full houses. Since this undercounts by a factor of two, you can check that if you multiply this by two, you get the correct answer above. 
\end{example}



Combinations are incredibly useful. We can use them in some rather surprising cases.

\begin{example}Consider the binary string \texttt{11000}. How many distinct orderings are there of the digits in this string?\\

At first, this does not appear to involve combinations at all, since we are looking for orderings, and combinations are used where order does not matter. Let's look at this problem in a different way. Consider the five-element set $A = \{a, b, c, d, e\}$. There are $\binom{5}{2}$ two-element subsets of $A$. One way to describe subsets of $A$ is to make a table. The columns of the table represent the elements of $A$: $a, b, c, d$, and $e$. Each row corresponds to a subset of $A$: a 1 indicates that the element is in the subset, and a 0 indicates that it is not. Here is an example table, where we depict two two-element subsets.

\begin{figure}[H]
\centering
\label{twoelementsubsets}
\begin{tabular}{lllll|l}
$a$ & $b$ & $c$ & $d$ & $e$ & subset \\
\hline
0 & 0 & 1 & 1 & 0 & $\{c, d\}$\\
1 & 0 & 1 & 0 & 0 & $\{a, c\}$\\
\end{tabular}
\end{figure}

Notice that two-element subsets match up exactly with orderings of \texttt{11000}! Thus we see that there are $\binom{5}{2}$ rearrangements of the binary string \texttt{11000}.
\end{example}

In general, if you have a string of length $n$ composed of two symbols, and you have $r$ of one symbol (thus $n-r$ of the other), the number of distinct orderings of the string is $\binom{n}{r}$. Equipped with this, let's return to the antenna example from the beginning of the section.

\begin{example}A communication system consists of $n$ antennas arranged in a line. Exactly $m$ out of the $n$ antennas are defective. The system is functional if no two consecutive antennas are defective. Assuming that each linear arrangement of the antennas is equally likely, what is the probability that the system will be functional?\\

First we need to figure out how many possible arrangements there are, i.e. the size of our sample space. If we use the digit \texttt{1} to represent a functional antenna, and the digit \texttt{0} to represent a defective antenna, then each arrangement can be represented by a binary string of length $n$ consisting of $m$ 0s and $n - m$ 1s. Thus from what we learned above, there are $\binom{n}{m}$ possible arrangements. \\

How many of these arrangements are functional? Let's draw a picture! Imagine we have the $n - m$ functional antennas in a line. We will represent each functional antenna by the symbol *, and the spaces between the functional antennas by a vertical bar $|$. We will also place a space to the far right and to the far right:
\[
| * | * | * | * | * | * | \dots | * | * |
\]
For the system to be functional, each space $|$ can contain at most one defective antenna. There are $n - m + 1$ spaces to choose from, and $m$ defective antennas to place. Thus there are $\binom{n-m+1}{m}$ functional arrangements. Dividing this by the total number of arrangements $\binom{n}{m}$, the probability that the system is functional is:
\[
\frac{ \binom{n-m+1}{m}}{\binom{n}{m}}
\]
For a sanity check, let's calculate this for the case where $n = 4$ and $m = 2$, which we did manually above:
\[
\frac{ \binom{n-m+1}{m}}{\binom{n}{m}} = \frac{\binom{3}{2}}{\binom{4}{2}} = \frac{3}{6} = \frac{1}{2}
\]
\end{example}
This agrees with what we found above!

\begin{example}
I buy an assortment of 10 bagels from Bagel Gourmet. There are five types of bagels to choose from: everything, onion, poppy, raisin, and sesame. How many different assortments of bagels can I bring home? Assume there are at least 10 of each kind of bagel in the store.\\

We can model the problem in a way that lets us use combinations. Imagine you buy the bagels in a special box. The bagels are arranged in a line, and are always in alphabetical order: everything, onion, poppy, raisin, sesame. There are four dividers, one between each bagel type, and if a bagel type is not present, we just put the dividers next to each other. Here is an example of three bagel boxes with their dividers. Bagels are designated by * and dividers by $|$.
\begin{figure}[H]
\begin{tabular}{ll}
$* * | * | * * * | * * | * *$ & 2 everything, 1 onion, 3 poppy, 2 raisin, 2 sesame \\
$* * * | | * * * | | * *$ & 3 everything, 3 poppy, 2 sesame\\
$* * * * * * * * * * | | | |$ & 10 everything\\
\end{tabular}
\end{figure}
From the picture, we see that an assortement of 10 bagels can be represented as a linear string of 14 symbols: 10 * representing the bagels and 4 $|$ representing the dividers. The number of assortments is the same as the number of distinct orderings of this linear string: $\binom{14}{4} = 1001$
\end{example}

One final application of combinations is to the binomial theorem. The binomial theorem gives us a nice expression for expanding the binomial $(x+y)^n$, where $n$ is a positive integer.

\begin{framed}
\emph{The binomial theorem}\\
  \rule{\dimexpr\linewidth-2\fboxsep-2\fboxrule}{.1pt} \\
\[
(x+y)^n = \sum_{r=0}^n \binom{n}{r}x^r y^{n-r}
\]
\end{framed}
Due to their appearance in this theorem, the coefficients $\binom{n}{r}$ are often called \emph{binomial coefficients}. Rather than give the usual proof by induction, we will argue this is true using what we know about combinations. Consider the product:
\[
(x_1 + y_1)(x_2 + y_2)\cdots(x_n + y_n)
\]
If we multiply this out, we get a sum of $2^n$ terms, each of which is the product of $n$ factors. To see this is the case, to get a term in the sum, we pick either $x_i$ or $y_i$ from each of the $n$ binomials $(x_i + y_i)$ and multiply them together. There are $2^n$ such terms, since for each binomial we have two possible choices and there are $n$ total binomials. How many of these $2^n$ terms have exactly $r$ of the $x_i$ terms and $n-r$ of the $y_i$ terms? We can think of each term as a string of length $n$ composed of $r$ $x$'s and $n-r$ $y$'s. By what we learned above, there are $\binom{n}{r}$ such terms. Taking $x_i = x$ and $y_i = y$ for all $i$, we obtain the binomial theorem.

\subsubsection{Multinomials}
\begin{framed}
The number of ways of dividing $n$ objects into $k$ distinct groups containing $n_1, dots, n_k$ objects each, where each object appears in exactly one group, and $n_1 + \cdots + n_k = n$ is given by:
\[
\binom{n}{n_1 \: n_2\: \cdots \:n_k} = \frac{n!}{n_1!\:n_2!\:\cdots\:n_k!}
\]
The term on the left is called a \emph{multinomial coefficient}
\end{framed}
To see why this is the case, we will use the following argument:
\begin{enumerate}
\item For the first group, we choose $n_1$ out of $n$ items. Since order does not matter, there are $\binom{n}{n_1}$ ways to do this.
\item For each choice in step 1, there are $\binom{n - n_1}{n_2}$ choices for the second group, since we have already taken $n_1$ items out of the larger group.
\item For each choice in step 1 and step 2, there are $\binom{n - n_1 - n_2}{n_3}$ choices for the third group, since we already have taken $n_1 + n_2$ items out of the larger group.
\item Repeat this until we get to the $k$th group, where there are $\binom{n - n_1 - \dots - n_{k-1}}{n_k}$ choices for the $k$th group given all the previous choices
\item Multiply these together to get:
\begin{align*}
&\binom{n}{n_1 \: n_2\: \cdots \:n_k} = \binom{n}{n_1} \binom{n - n_1}{n_2} \binom{n - n_1 - n_2}{n_3} \cdots \binom{n - n_1 - \dots - n_{k-1}}{n_k} \\
&= \frac{n!}{n_1! (n - n_1)!}\:\frac{(n - n_1)!}{n_2!(n - n_1 - n_2!)}\:\frac{(n - n_1 - n_2)!}{n_3!(n - n_1 - n_2 - n_3!)}\cdots\frac{n - n_1 - \dots - n_{k-1}!}{n_k! (n - n_1 - \dots - n_k)!}\\
&= \frac{n!}{n_1! \: n_2! \: n_3! \cdots n_k! (n - n_1 - \dots - n_k! )} \\
&= \frac{n!}{n_1! \: n_2! \: n_3! \cdots n_k! \: 0! } \\
&= \frac{n!}{n_1! \: n_2! \: n_3! \cdots n_k! }
\end{align*}
\end{enumerate}
where, after some very satisfying cancellation, we have used the fact that $n_1 + \dots + n_k = n$ and $0! = 1$.\\

If you want to think about this another way, you can use the following reasoning. Imagine we have $n$ objects and want to divide them into groups as above. Imagine we have a special box which contains the $n$ objects in a line. Divide the box into sections using dividers, so that the first section contains $n_1$ objects, the second $n_2$ objects, all the way down to the $k$th section which contains $k$ objects. For example, if we had $n = 10$ with four groups $n_1 = 2, n_2 = 4, n_3 = 1, n_4 = 3$, our box looks like:
\[
* * | * * * * | * | * * *
\]
where * represents a space to put an objects, and $|$ is a section divider. There are $n!$ permutations of $n$ objects, thus there are $n!$ distinct ways to place the $n$ objects into the box. Since we are dividing our objects into groups, order does not matter within each group. The first section in the box represents the first group, so since order does not matter within that section, we divide by $n_1!$, the number of permutations of $n_1$ objects. Repeating this for each section, we divide by each $n_i!$ in turn to obtain our multinomial coefficient. 

\begin{example}A group of 15 international relations students will be divided into three groups. Each group consists of 15 students and is assigned a different country (Azerbaijan, Belarus, and Chechnya) on which to do a group final presentation. How many such divisions are possible:\\

Since we are are dividing 15 people into 3 distinct groups of 5, we can use the multinomial coefficient to determine the number or arrangements:
\[
\binom{15}{5 \: 5\: 5} = \frac{15!}{5!\:5!\:5!} = 756,756
\]
\end{example}

\begin{example}Now suppose we are dividing a group of 15 international relations students into three groups of 5 students each for a final project of the each group's choosing. How is this problem different from the previous one? How many different arrangements are possible?\\

In the previous example, the three groups were distinct, so it matters which one a student is assigned to. In this case, the three groups are identical. Thus since the order of the three groups does not mater, we divide the previous result by $3!$, which is the number of ways of ordering three items (in this case, the items are the three groups).
\[
\frac{ \binom{15}{5 \: 5\: 5}}{3!} = \frac{15!}{5!\:5!\:5!\:3!} = 126,126
\]
\end{example}

\begin{example}How many different, distinct rearrangements are there of the word MISSISSIPPI?\\

We are looking for all distinct orderings of a string with four distinct symbols. If there were only two distinct symbols, we could use binomial coefficients, so we suspect multinomial coefficients may be in play here. Once again, consider a set with eleven elements in them, which we will label with the integers 1 - 11. We want to divide those elements into four subsets with 1, 2, 4, and 4 elements (respectively). We will label those subsets by the letters M, P, I, and S (no coincidence here!). Let's make a table, where each row is a subset, and letters M, I, S, and P in the row indicate which subset each element belongs to.

\begin{figure}[H]
\centering
\label{mississippi}
\begin{tabular}{lllllllllll}
$1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$ & $11$ \\
\hline
M & I & S & S & I & S & S & I & P & P & I \\
I & S & I & P & M & P & S & I & S & S & I \\
\end{tabular}
\end{figure} 

Note that there is a perfect correspondance between each appropriate collection of four subsets and each arrangement of MISSISSIPPI. Thus we can use the multinomial coefficient for this problem, and we get that the total number of distinct rearrangements is:
\[
\binom{11}{1 \: 2\: 4 \: 4} = \frac{11!}{1!\:2!\:4!\:4!} = 34,650
\]


\end{example}

\end{document}
